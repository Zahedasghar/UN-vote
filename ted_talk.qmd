---
title: "Text mining with tidy data principles"
subtitle:  "Learn how to use tidytext!"

author: "Julia Silge"
format: html
  
---


```{r setup, include=FALSE}
#library(learnr)
library(tidyverse)
library(tidytext)
library(stopwords)
library(ggrepel)
library(scales)
library(plotly)
library(here)
library(gradethis)

theme_set(theme_light())

ted_talks <- read_rds(here::here("data", "ted_talks.rds"))
tidy_talks <- ted_talks %>% unnest_tokens(word, text)

#shakespeare <- read_rds(here::here("data", "shakespeare.rds"))
#tidy_shakespeare <- shakespeare %>%
 # group_by(title) %>%
  #mutate(linenumber = row_number()) %>%
  #ungroup() %>%
  #unnest_tokens(word, text)

#nyt_headlines <- read_rds(here::here("data", "nyt_headlines.rds"))
#tidy_nyt <- nyt_headlines %>%
 # mutate(id = row_number()) %>%
  #unnest_tokens(word, headline)


#song_lyrics <- read_rds(here::here("data", "song_lyrics.rds"))
#tidy_lyrics <- song_lyrics %>% unnest_tokens(word, lyrics)
```

## 1. Introduction



Text data sets are diverse and ubiquitous, and **tidy data principles** provide an approach to make text mining easier, more effective, and consistent with tools already in wide use. In this tutorial, you will develop your text mining skills using the [tidytext](https://juliasilge.github.io/tidytext/) package in R, along with other [tidyverse](https://www.tidyverse.org/) tools. You will apply these skills in several case studies, which will allow you to: 
  
  - practice important data handling skills, 
- learn about the ways text analysis can be applied, and 
- extract relevant insights from real-world data.

### Working through this tutorial

Throughout this tutorial, you will see code exercises that look like this:
  
```{r }
# load the tidytext package

```

```{r }
# load the tidytext package
library(tidytext)
```

```{r }
grade_code("Be sure to click \"Submit Answer \" on exercises throughout the tutorial because there are hints, answers, and other content available to you after you submit.")
```

You can type in these code exercises. **Give it a try now!** If you mess up, click "Start Over" to get back to the original state. Use the "Run Code" button to see what happens, and click on "Solution" to check out the solution.

In the exercise above, type `library(tidytext)` and click "Submit Answer".

This tutorial is organized into **four case studies**, each with its own data set:
  
- transcripts of TED talks
- a collection of comedies and tragedies by Shakespeare
- one month of newspaper headlines
- song lyrics spanning five decades

These case studies demonstrate how you can use text analysis techniques with diverse kinds of text!
  
### Prerequisites
  
  To get the most from this tutorial, you should have some familiarity with R and [tidyverse](https://www.tidyverse.org/) functions like those from dplyr and ggplot2. If you have read [*R for Data Science*](https://r4ds.had.co.nz/) by Hadley Wickham and Garrett Grolemund, you are good to go!
  
## 2. Thank you for coming to my TED talk {data-progressive=TRUE}
  
  <iframe src="https://giphy.com/embed/495RS6uWPjq008HACu" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/usnationalarchives-applause-hollywood-audience-495RS6uWPjq008HACu">via GIPHY</a></p>
  
  The first case study of this tutorial uses a data set of TED talks created by Katherine M. Kinnaird and John Laudun for their paper ["TED Talks as Data"](https://doi.org/10.22148/16.042). The specific talks we are using are from the main TED event, and the data set was curated in the summer of 2018.

There are two main pieces of R software we will use in our text analysis work throughout this tutorial, the [tidyverse](https://www.tidyverse.org/) metapackage and [tidytext](https://juliasilge.github.io/tidytext/). To clarify for yourself what tools you are using, load the two packages below (first tidyverse, and then tidytext) by replacing the `___` with the package names.



```{r }
# load the tidyverse and tidytext packages
library(tidyverse)
library(tidytext)
```



### TED talk transcripts

The TED talk transcripts are available to you in a dataframe called `ted_talks`. There are three variables in this data set:
  
  - `talk_id`: the identifier from the TED website for this particular talk
- `text`: the text of this TED talk
- `speaker`: the main or first listed speaker (some TED talks have more than one speaker)



```{r ted-talks-solution}
# glimpse `ted_talks` to see what is in the data set
glimpse(ted_talks)
```


### How to tidy text data

The `text` data is currently in a dataframe, but it is not **tidy** in the sense of being compatible with tidy tools. We need to transform it so that it is in a different format, with **one observation per row**.

When we do text analysis, the observations we are interested in aren't the whole talks at once, but rather individual _tokens_. A **token** is a meaningful unit of text for analysis; in many cases, this just means a single word. The process of **tokenization** identifies and breaks apart text into individual tokens. You can use [tidytextâ€™s `unnest_tokens()` function](https://juliasilge.github.io/tidytext/reference/unnest_tokens.html) to accomplish all of this at once, both the tidying and the tokenization.


```{r }
tidy_talks <- ted_talks %>% 
  unnest_tokens(word, text)
```



### Tidy TED talks

The `unnest_tokens()` function transforms non-tidy text data into tidy text data. It takes three arguments: 

- the input dataframe that contains your text (often you will use the pipe `%>%` to send this argument to `unnest_tokens()`),
- the output column that you want to unnest *to*, and 
- the input column that you want to unnest *from*.

```{r display-tidy-ted, echo=TRUE}
tidy_talks
```

What did `unnest_tokens()` do here? Instead of having `r nrow(ted_talks)` rows and reading each talk across the line in `text`, we now have `r scales::comma(nrow(tidy_talks))` rows and can read each talk _down_ the column in `word`. We have **tokenized** and **tidied** the text, as well as a few other transformations:

- Other columns have been retained.
- Punctuation has been stripped out.
- Words have been converted to lower-case.

These are defaults in the function that can be changed, if not appropriate to your analysis.

### Tokenize to bigrams

We said before that tokenization is the process of identifying and breaking apart text into **tokens**, meaningful units of text; those meaningful units of text are most often single words in text analysis but they do not have to be! We can move beyond single words to other kinds of tokens, like **n-grams**. An n-gram is a consecutive sequence of `n` words. Let's look at these TED talks and tokenize to bigrams, n-grams of order 2.

- Use the same function for tokenizing and tidying to create the bigrams.



```{r ted-bigram-solution}
ted_bigrams <- ted_talks %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

ted_bigrams
```


### Most common TED talk words

Let's go back to single words. Now that our data in a tidy format, a whole world of analysis opportunity has opened up for us. We can start by computing term frequencies in just one line. What are the **most common words** in these TED talks? 

- Use `count()` to find the most common words.



```{r top-ted-solution}
tidy_talks %>%
  count(word, sort = TRUE)
```


### Removing stop words

Words like "the", "and", and "to" that aren't very interesting for a text analysis are called **stop words**. Often the best choice is to remove them. The tidytext package provides access to stop word lexicons, with a default list and then other options and other languages.

- First, run the code the way it is.
- Next, try out the `language` argument, which takes two-letter language abbreviations like `"es"`.



When text data is in a tidy format, stop words can be removed using an [`anti_join()`](https://dplyr.tidyverse.org/reference/filter-joins.html). This type of join will "filter" or remove items that are in the right-hand side, keeping those in the left-hand side.

- Within the `anti_join()`, add the call to `get_stopwords()`.
- Add the arguments to `count()` so we count up words and sort them with the largest groups at the top.
`

```{r stop-word-solution}
tidy_talks %>%
  anti_join(get_stopwords()) %>%
  count(word, sort = TRUE)
```


### Visualize top words

Because we are using tidyverse tools, we can fluently pipe from the kind of code we just wrote straight to ggplot2 functions. One of the significant benefits of using tidy data principles is consistency with widely-used tools that are broadly supported.

- Remove stop words (the default list) via an `anti_join()`.
- Create a plot with `n` on the x-axis and `word` on the y-axis.


```{r stop-word-viz-solution}
tidy_talks %>%
  # remove stop words
  anti_join(get_stopwords()) %>%
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20) %>%
  mutate(word = reorder(word, n)) %>%
  # put `n` on the x-axis and `word` on the y-axis
  ggplot(aes(n, word)) +
  geom_col()
```


### Compare TED talk vocabularies

One of my favorite approaches to text analysis is to compare how different people or groups use language. There are lots of different ways to do this, but you can start with plain old word counts! Let's look at two TED talk speakers, [Jane Goodall](https://en.wikipedia.org/wiki/Jane_Goodall) and [Temple Grandin](https://en.wikipedia.org/wiki/Temple_Grandin), and count up the words they used in their TED talks.

*If you want to explore other speakers, switch out for different speakers' names from the data set and hit "Run Code", after finishing the exercise.*
  
  - Use `filter()` to keep only the words spoken by Jane Goodall and Temple Grandin.
- Remove the default list of stop words.
- Use `count()` with two arguments to count up the term frequencies by `speaker` and `word`. (These first three steps could actually be completed in any order but this makes most sense to me.)
- Come back and `filter()` again to only keep words spoken at least 10 times by both women.

The function [`pivot_wider()` from tidyr](https://tidyr.tidyverse.org/reference/pivot_wider.html) pivots the long, tidy dataframe to a wide dataframe so we can more easily compare the two speakers' word counts.



```{r ted-final-pivot-solution}
tidy_talks %>%
  filter(speaker %in% c("Jane Goodall", "Temple Grandin")) %>%
  # remove stop words
  anti_join(get_stopwords()) %>%
  # count with two arguments
  count(speaker, word) %>%
  group_by(word) %>%
  filter(sum(n) > 10) %>%
  ungroup() %>%
  pivot_wider(names_from = "speaker", values_from = "n", values_fill = 0) 
```


```{r ted-final-viz-solution}
library(ggrepel)

tidy_talks %>%
  filter(speaker %in% c("Jane Goodall", "Temple Grandin")) %>%
  anti_join(get_stopwords()) %>%
  count(speaker, word) %>%
  group_by(word) %>%
  filter(sum(n) > 10) %>%
  ungroup() %>%
  pivot_wider(names_from = "speaker", values_from = "n", values_fill = 0) %>%
  ggplot(aes(`Jane Goodall`, `Temple Grandin`)) +
  geom_abline(color = "gray50", linewidth = 1.2, alpha = 0.8, lty = 2) +
  # use the special ggrepel geom for nicer text plotting
  geom_text_repel(aes(label = word)) +
  coord_fixed()
```



